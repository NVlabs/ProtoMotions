# @package _global_

num_envs: 4
agent:
  _target_: protomotions.agents.masked_mimic.agent_no_vae.MaskedMimicNoVae
  _recursive_: False
  config:
    # Setup basic actor-critic structure
    model:
      _target_: protomotions.agents.masked_mimic.model_no_vae.DeterministicOutputModel
      _recursive_: False
      config:
        trunk:
          _target_: protomotions.agents.common.mlp.MultiHeadedMLP
          _recursive_: False
          num_out: ${robot.number_of_actions}
          config:
            input_models:
              self_obs:
                _target_: protomotions.agents.common.common.Flatten
                _recursive_: False
                num_in: ${robot.self_obs_size}
                num_out: ${.num_in}
                config:
                  obs_key: self_obs
                  normalize_obs: True
                  norm_clamp_value: 5
              encoder_out:
                _target_: protomotions.agents.common.common.Flatten
                _recursive_: False
                num_in: ${agent.config.model.config.encoder.config.latent_dim}
                num_out: ${.num_in}
                config:
                  obs_key: encoder_out
                  normalize_obs: False
                  norm_clamp_value: null
            trunk:
              _target_: protomotions.agents.common.mlp.MLP
              _recursive_: False
              num_out: ${robot.number_of_actions}
              config:
                layers:
                  - units: 1024
                    activation: relu
                    use_layer_norm: false
                  - units: 1024
                    activation: relu
                    use_layer_norm: false
                  - units: 512
                    activation: relu
                    use_layer_norm: false

        optimizer:
          _target_: torch.optim.Adam
          lr: 2e-5
    # PPO parameters
    gamma: 0.99
    tau: 0.95
    e_clip: 0.2
    normalize_values: false
    normalize_advantage: false
    # Mimic parameters
    gradient_clip_val: 50.0
    clip_critic_loss: True
    eval_metric_keys: ["cartesian_err", "gt_err", "dv_rew", "kb_rew", "lr_rew", "rv_rew", "rav_rew", "gr_err", "gr_err_degrees"]
    eval_length: null
    eval_num_episodes: 1

    # Early termination
    training_early_termination:
      early_terminate_cart_err: null
      early_terminate_success_rate: null
    # general params
    num_steps: 32
    fail_on_bad_grads: False
    check_grad_mag: True
    batch_size: 1024
    task_reward_w: 1.0
    num_mini_epochs: 6
    max_eval_steps: null
    eval_metrics_every: 500
    num_games: null
    manual_save_every: 10
    max_epochs: ${eval:${training_max_steps}//${ngpu}//${num_envs}//${.num_steps}}

    expert_model_path: ???

    extra_inputs:
      mimic_target_poses: true
      masked_mimic_target_poses: true
      masked_mimic_target_bodies_masks: true
      masked_mimic_target_poses_masks: true
      motion_text_embeddings: true
      motion_text_embeddings_mask: true
      historical_pose_obs: true

eval_overrides:
  agent:
    config:
      expert_model_path: null
      vae:
        noise_type: "zeros"  # at inference, by default, play from the mean
